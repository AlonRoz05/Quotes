{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f6607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers \"datasets[s3]\" --upgrade\n",
    "!pip install sagemaker --upgrade\n",
    "!pip install sagemaker-experiments\n",
    "!pip install evaluate\n",
    "!pip install rouge-score\n",
    "!pip -q install transformers datasets sagemaker --upgrade\n",
    "!pip -q install widgetsnbextension ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a37c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a6ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session_bucket = \"quotes-hf-aws\"\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45911aab-c961-4a59-a76e-51839138b00f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_name = 'google/flan-t5-large'\n",
    "dataset_name = 'Rozi05/quotes_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6b50e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(dataset_name, split='train').train_test_split(test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113cf74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = dataset[\"train\"], dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efa742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "max_source_length = 275\n",
    "max_target_length = 512\n",
    "\n",
    "def preprocess_function(sample, padding=\"max_length\"):\n",
    "    inputs = [\"Write a motivational quote that: \" + (tag + \", describes it best.\") for tag in sample[\"tags\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length = max_source_length, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=sample[\"quote\"], max_length = max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6482c1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"index\", \"quote\", \"tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfe41f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "s3_prefix = 'samples/datasets/quotes_dataset'\n",
    "\n",
    "dataset_input_path = \"s3://{}/{}\".format(sagemaker_session_bucket, s3_prefix)\n",
    "train_input_path = \"{}/train\".format(dataset_input_path)\n",
    "valid_input_path = \"{}/validation\".format(dataset_input_path)\n",
    "\n",
    "# save datasets to s3\n",
    "tokenized_dataset[\"train\"].save_to_disk(train_input_path, fs=s3)\n",
    "tokenized_dataset[\"test\"].save_to_disk(valid_input_path, fs=s3)\n",
    "    \n",
    "print(dataset_input_path)\n",
    "print(train_input_path)\n",
    "print(valid_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116632e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "hyperparameters = {\n",
    "    'model_name_or_path':'google/flan-t5-large',\n",
    "    'output_dir':'/opt/ml/model',\n",
    "    'do_train':True,\n",
    "    'dataset_name':'Rozi05/quotes_dataset',\n",
    "    \"epochs\": 1,\n",
    "    \"learning-rate\": 1e-6,\n",
    "    \"train-batch-size\": 1,\n",
    "    \"eval-batch-size\": 8,\n",
    "}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.26.0'}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    role=sagemaker.get_execution_role(),\n",
    "\n",
    "    entry_point=\"train.py\",\n",
    "    dependencies=[\"requirements.txt\"],\n",
    "    hyperparameters=hyperparameters,\n",
    "\n",
    "    transformers_version=\"4.26.0\",\n",
    "    pytorch_version=\"1.13.1\",\n",
    "    py_version=\"py39\",\n",
    "    instance_type=\"ml.p3.16xlarge\",\n",
    "    instance_count=1,\n",
    "    distribution={\"smdistributed\": {\"dataparallel\": {\"enabled\": True}}},\n",
    ")\n",
    "\n",
    "# starting the train job\n",
    "huggingface_estimator.fit({\"train\": train_input_path, \"valid\": valid_input_path})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
