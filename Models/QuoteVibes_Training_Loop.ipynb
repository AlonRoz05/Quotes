{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-d6m1ugoulIx"},"outputs":[],"source":["!pip install evaluate\n","!pip install transformers\n","!pip install datasets\n","!pip install rouge-score\n","!pip install nltk\n","!pip install evaluate\n","!pip install accelerate -U"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3TGA1KSgyA5u"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aI3dMoVMZlg-"},"outputs":[],"source":["from datasets import load_dataset, concatenate_datasets\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n","\n","import nltk\n","import evaluate\n","import numpy as np\n","\n","from nltk.tokenize import sent_tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u9y_1ZyBZmIY"},"outputs":[],"source":["model_id = 'google/flan-t5-base'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Sb0BwUMZp_I"},"outputs":[],"source":["train_dataset = load_dataset(\"csv\", data_files=\"train_quotes_dataset.csv\", sep=\",\")\n","test_dataset = load_dataset(\"csv\", data_files=\"test_quotes_dataset.csv\", sep=\",\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GpsEcTTh57HA"},"outputs":[],"source":["tokenized_inputs = concatenate_datasets([train_dataset[\"train\"], test_dataset[\"train\"]]).map(lambda x: tokenizer(x[\"tags\"], truncation=True), batched=True, remove_columns=['index', 'quote', 'tags'])\n","max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n","\n","tokenized_targets = concatenate_datasets([train_dataset[\"train\"], test_dataset[\"train\"]]).map(lambda x: tokenizer(x[\"quote\"], truncation=True), batched=True, remove_columns=['index', 'quote', 'tags'])\n","max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GgHRDTXdZsih"},"outputs":[],"source":["def preprocess_function(sample, padding=\"max_length\"):\n","    inputs = [\"Generate motivational quote about: \" + item for item in sample[\"tags\"]]\n","    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n","\n","    labels = tokenizer(text_target=sample[\"quote\"], max_length=max_target_length, padding=padding, truncation=True)\n","\n","    if padding == \"max_length\":\n","        labels[\"input_ids\"] = [\n","            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n","        ]\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iI46XIJRgTI_"},"outputs":[],"source":["tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"index\", \"quote\", \"tags\"])\n","tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=[\"index\", \"quote\", \"tags\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4c3yhVU41vl9"},"outputs":[],"source":["nltk.download(\"punkt\")\n","metric = evaluate.load(\"rouge\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfQEPT3BZzJz"},"outputs":[],"source":["def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","\n","    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n","    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n","\n","    return preds, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Gtivr3f1idc"},"outputs":[],"source":["def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    result = {k: round(v * 100, 4) for k, v in result.items()}\n","\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2G8Qbltyb1u"},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(\n","    tokenizer,\n","    model=model,\n","    label_pad_token_id=-100,\n","    pad_to_multiple_of=8\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1A5w39mafAx"},"outputs":[],"source":["args = Seq2SeqTrainingArguments(\n","    output_dir = \"test\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    weight_decay = 0.01,\n","    learning_rate = 5e-5,\n","    save_total_limit = 2,\n","    num_train_epochs = 10,\n","    per_device_train_batch_size = 16,\n","    per_device_eval_batch_size = 16,\n","    load_best_model_at_end = True,\n","    predict_with_generate = True,\n","    fp16 = False,\n","    push_to_hub = True,\n","    hub_model_id = \"QuoteVibes_Model_Trained\"\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model = model,\n","    args = args,\n","    data_collator = data_collator,\n","    train_dataset = tokenized_train_dataset[\"train\"],\n","    eval_dataset = tokenized_test_dataset[\"train\"],\n","    compute_metrics = compute_metrics\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gtAUWfqoahKn"},"outputs":[],"source":["trainer.train()\n","\n","trainer.push_to_hub(\"End of training\")\n","\n","trainer.save_model(\"Models\")\n","tokenizer.save_pretrained(\"Tokenizer\")\n","\n","model.push_to_hub(\"QuoteVibes_Model_Trained\")\n","tokenizer.push_to_hub(\"QuoteVibes_Model_Trained\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPS4v3PWyKhll8xjElsrSe7","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
